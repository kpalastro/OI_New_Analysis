"""
Reinforcement learning framework for OI Gemini (Phase 2).

Provides infrastructure for RL-based trading strategies and execution optimization.
"""
from __future__ import annotations

import logging
from dataclasses import dataclass
from typing import Any, Dict, List, Optional, Tuple

import numpy as np
import pandas as pd

try:
    from stable_baselines3 import PPO, DQN
    from stable_baselines3.common.env_util import make_vec_env
    from gymnasium import spaces  # Use gymnasium if available, else gym
    import gymnasium as gym
    SB3_AVAILABLE = True
except ImportError:
    SB3_AVAILABLE = False
    PPO = None
    DQN = None
    make_vec_env = None
    spaces = None
    gym = None

LOGGER = logging.getLogger(__name__)


@dataclass
class RLState:
    """State representation for RL agent."""
    features: np.ndarray
    current_position: float  # Current position size (-1 to 1)
    portfolio_value: float
    timestamp: str


@dataclass
class RLAction:
    """Action space for RL agent."""
    signal: int  # -1=SELL, 0=HOLD, 1=BUY
    position_size: float  # Fraction of capital (0 to 1)


class TradingEnvironment(gym.Env if gym else object):
    """
    Gymnasium-compatible environment for RL training using backtest engine.
    """
    
    def __init__(
        self,
        exchange: str,
        features_df: Any,  # pd.DataFrame
        initial_capital: float = 1_000_000.0,
    ):
        if not SB3_AVAILABLE or gym is None:
            raise ImportError("Gymnasium is required for TradingEnvironment")
        
        self.exchange = exchange
        self.features_df = features_df
        self.initial_capital = initial_capital
        self.current_step = 0
        self.current_position = 0.0
        self.portfolio_value = initial_capital
        self.done = False
        
        if features_df is None or len(features_df) == 0:
            raise ValueError("Features dataframe is empty")
        
        # Define observation space (state vector)
        # State includes: features + position + portfolio_value
        feature_cols = [col for col in self.features_df.columns 
                       if col not in ['timestamp', 'target', 'future_return']]
        n_features = len(feature_cols)
        state_dim = n_features + 2  # features + position + portfolio_value
        
        self.observation_space = spaces.Box(
            low=-np.inf, 
            high=np.inf, 
            shape=(state_dim,), 
            dtype=np.float32
        )
        
        # Define action space
        # For PPO: Continuous Box action space
        # For DQN: Discrete action space (we'll use MultiDiscrete)
        # Action: [signal (-1, 0, 1), position_size (0 to 1)]
        # Default to continuous (PPO), can be changed to discrete (DQN) via set_action_space()
        self._action_space_type = "continuous"  # or "discrete"
        
        # Continuous action space (for PPO)
        self.action_space = spaces.Box(
            low=np.array([-1.0, 0.0], dtype=np.float32),
            high=np.array([1.0, 1.0], dtype=np.float32),
            dtype=np.float32
        )
        
        # Discrete action space (for DQN)
        # Signal: 3 options (-1, 0, 1)
        # Position size: 5 bins (0.0, 0.25, 0.5, 0.75, 1.0)
        # Total: 3 * 5 = 15 discrete actions
        # DQN only supports Discrete, not MultiDiscrete, so we flatten to a single Discrete(15)
        self.discrete_action_space = spaces.Discrete(15)  # 3 signals Ã— 5 position sizes
        self.position_size_bins = np.array([0.0, 0.25, 0.5, 0.75, 1.0])
        self.signal_map = [-1, 0, 1]  # Map signal index to actual signal
    
    def set_action_space(self, space_type: str):
        """Switch between continuous and discrete action spaces."""
        if space_type == "discrete":
            self.action_space = self.discrete_action_space
            self._action_space_type = "discrete"
        elif space_type == "continuous":
            self.action_space = spaces.Box(
                low=np.array([-1.0, 0.0], dtype=np.float32),
                high=np.array([1.0, 1.0], dtype=np.float32),
                dtype=np.float32
            )
            self._action_space_type = "continuous"
        else:
            raise ValueError(f"Unknown action space type: {space_type}")
    
    def reset(self, seed=None, options=None):
        """Reset environment to initial state."""
        if seed is not None:
            np.random.seed(seed)
        
        self.current_step = 0
        self.current_position = 0.0
        self.portfolio_value = self.initial_capital
        self.done = False
        
        observation = self._get_state()
        info = {}
        return observation, info
    
    def step(self, action):
        """
        Execute action and return next state, reward, done, truncated, info.
        
        Args:
            action: Array-like [signal, position_size] or RLAction
        
        Returns:
            Tuple of (observation, reward, terminated, truncated, info)
        """
        if self.done:
            obs = self._get_state()
            return obs, 0.0, True, False, {}
        
        # Convert action to RLAction format
        if self._action_space_type == "discrete":
            # Discrete action: single integer 0-14
            # Decode: action = signal_idx * 5 + position_size_idx
            action_int = int(action)
            signal_idx = action_int // 5  # 0, 1, or 2
            position_size_idx = action_int % 5  # 0, 1, 2, 3, or 4
            
            # Map signal index to actual signal
            signal = self.signal_map[signal_idx]
            
            # Map position size index to actual position size
            position_size = float(self.position_size_bins[position_size_idx])
        else:
            # Continuous action space
            if isinstance(action, (list, np.ndarray)):
                signal = np.clip(action[0], -1.0, 1.0)
                position_size = np.clip(action[1], 0.0, 1.0)
                # Discretize signal to -1, 0, or 1
                if signal < -0.33:
                    signal = -1
                elif signal > 0.33:
                    signal = 1
                else:
                    signal = 0
            elif isinstance(action, RLAction):
                signal = action.signal
                position_size = action.position_size
            else:
                # Fallback: assume single value is signal
                signal = int(np.clip(action, -1, 1))
                position_size = 0.5
        
        # Get current price and future return
        current_row = self.features_df.iloc[self.current_step]
        
        # Calculate PnL from action
        position_change = position_size - self.current_position
        if signal != 0 and abs(position_change) > 0.01:  # Only if significant change
            # Transaction cost (assume 0.02% per trade)
            transaction_cost = abs(position_change) * self.portfolio_value * 0.0002
            self.portfolio_value -= transaction_cost
        
        # Update position based on signal
        if signal != 0:
            self.current_position = position_size if signal > 0 else -position_size
        # If signal is 0 (HOLD), keep current position
        
        # Calculate reward (PnL normalized by capital)
        if self.current_step + 1 < len(self.features_df):
            next_return = self.features_df.iloc[self.current_step + 1].get('future_return', 0.0)
            pnl = self.current_position * next_return * self.portfolio_value
            reward = pnl / self.initial_capital  # Normalized reward
        else:
            reward = 0.0
        
        self.current_step += 1
        terminated = self.current_step >= len(self.features_df) - 1
        truncated = False  # Not using time limits
        self.done = terminated
        
        info = {
            'portfolio_value': self.portfolio_value,
            'position': self.current_position,
            'step': self.current_step,
        }
        
        observation = self._get_state()
        return observation, reward, terminated, truncated, info
    
    def _get_state(self) -> np.ndarray:
        """Extract state vector from current features."""
        if self.current_step >= len(self.features_df):
            # Return zeros if out of bounds
            feature_cols = [col for col in self.features_df.columns 
                           if col not in ['timestamp', 'target', 'future_return']]
            state = np.zeros(len(feature_cols) + 2, dtype=np.float32)
            return state
        
        row = self.features_df.iloc[self.current_step]
        # Extract numeric features (exclude timestamp, target, etc.)
        feature_cols = [col for col in self.features_df.columns 
                       if col not in ['timestamp', 'target', 'future_return']]
        
        # Get feature values, handling NaN
        feature_values = []
        for col in feature_cols:
            val = row.get(col, 0.0)
            if pd.isna(val):
                val = 0.0
            feature_values.append(float(val))
        
        state = np.array(feature_values, dtype=np.float32)
        
        # Add position and portfolio info
        state = np.append(state, [
            float(self.current_position), 
            float(self.portfolio_value / self.initial_capital)
        ])
        
        return state.astype(np.float32)


class RLStrategy:
    """Wrapper for RL-based trading strategy."""
    
    def __init__(
        self,
        exchange: str,
        model_path: Optional[str] = None,
        algorithm: str = "PPO",
    ):
        self.exchange = exchange
        self.algorithm = algorithm.upper()
        self.model = None
        self.model_loaded = False
        
        if not SB3_AVAILABLE:
            LOGGER.warning(f"[{exchange}] Stable Baselines3 not available, RL disabled")
            return
        
        if model_path:
            self._load_model(model_path)
    
    def _load_model(self, model_path: str) -> None:
        """Load trained RL model."""
        try:
            if self.algorithm == "PPO":
                self.model = PPO.load(model_path)
            elif self.algorithm == "DQN":
                self.model = DQN.load(model_path)
            else:
                LOGGER.error(f"[{self.exchange}] Unknown RL algorithm: {self.algorithm}")
                return
            
            self.model_loaded = True
            LOGGER.info(f"[{self.exchange}] RL model loaded: {model_path}")
        except Exception as e:
            LOGGER.error(f"[{self.exchange}] Failed to load RL model: {e}")
    
    def predict(self, state: np.ndarray) -> RLAction:
        """
        Generate action from state.
        
        Args:
            state: State vector
        
        Returns:
            RLAction with signal and position_size
        """
        if not self.model_loaded or self.model is None:
            return RLAction(signal=0, position_size=0.0)
        
        try:
            action, _ = self.model.predict(state, deterministic=True)
            # Map action to signal and position size
            # Assuming action is a single integer or array
            if isinstance(action, (list, np.ndarray)):
                signal = int(action[0]) if len(action) > 0 else 0
                position_size = float(action[1]) if len(action) > 1 else 0.0
            else:
                signal = int(action)
                position_size = 0.5  # Default position size
            
            return RLAction(signal=signal, position_size=position_size)
        except Exception as e:
            LOGGER.error(f"[{self.exchange}] RL prediction error: {e}")
            return RLAction(signal=0, position_size=0.0)


# --- Phase 2: Execution Optimization ---

@dataclass
class PlacementDetails:
    """Output from RL Executor."""
    price_offset: float # Ticks from mid-price
    aggression: int # 0=Passive, 1=Aggressive
    fill_probability_est: float


class ExecutionEnvironment(gym.Env if gym else object):
    """
    RL Environment for Order Execution optimization.
    Optimizes placement price and timing to minimize slippage.
    """
    def __init__(self, tick_data: List[Dict[str, Any]], target_quantity: int = 1):
        if not SB3_AVAILABLE:
            return
            
        self.tick_data = tick_data
        self.target_quantity = target_quantity
        self.current_step = 0
        
        # Action Space: [Price Offset (Continuous), Aggression (Discrete)]
        # Price Offset: -2.0 to +2.0 ticks
        # Aggression: 0 or 1
        self.action_space = spaces.Dict({
            "price_offset": spaces.Box(low=-2.0, high=2.0, shape=(1,), dtype=np.float32),
            "aggression": spaces.Discrete(2)
        })
        
        # State Space: [Spread, Imbalance, Volatility, Time Remaining]
        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(4,), dtype=np.float32)
        
    def reset(self, seed=None, options=None):
        self.current_step = 0
        return self._get_obs(), {}
        
    def step(self, action):
        # Unpack action
        price_offset = float(action["price_offset"][0])
        aggression = int(action["aggression"])
        
        # Simulate Fill
        tick = self.tick_data[self.current_step]
        mid_price = (tick['bid'] + tick['ask']) / 2.0
        spread = tick['ask'] - tick['bid']
        
        # Simplified simulation logic
        fill_price = mid_price + (price_offset * 0.05) # Assuming 0.05 tick
        
        # Calculate Reward (Implementation Shortfall)
        # Benchmark = Mid Price at arrival
        benchmark_price = (self.tick_data[0]['bid'] + self.tick_data[0]['ask']) / 2.0
        slippage = benchmark_price - fill_price # For buy (want lower)
        
        reward = slippage 
        if aggression == 1:
            # Pay spread but guaranteed fill (mostly)
            reward -= (spread / 2.0)
            
        self.current_step += 1
        done = True # Single step execution for now (simulating placement decision)
        
        return self._get_obs(), reward, done, False, {}
        
    def _get_obs(self):
        if self.current_step >= len(self.tick_data):
            return np.zeros(4)
            
        tick = self.tick_data[self.current_step]
        # Calculate features
        spread = tick['ask'] - tick['bid']
        imbalance = tick['bid_size'] / (tick['ask_size'] + 1e-9)
        vol = 0.0 # Placeholder
        time_left = 1.0 # Placeholder
        
        return np.array([spread, imbalance, vol, time_left], dtype=np.float32)


class RLExecutor:
    """
    RL Agent for trade execution.
    Decides how to place orders (Limit price, Aggression) based on market microstructure.
    """
    def __init__(self, exchange: str, model_path: Optional[str] = None):
        self.exchange = exchange
        self.model = None
        self.is_ready = False
        
        if SB3_AVAILABLE and model_path:
            try:
                self.model = PPO.load(model_path)
                self.is_ready = True
            except Exception as e:
                LOGGER.warning(f"[{exchange}] Failed to load RLExecutor model: {e}")

    def decide_placement(
        self, 
        symbol: str, 
        current_price: float, 
        spread: float, 
        imbalance: float
    ) -> PlacementDetails:
        """
        Decide how to place the order.
        """
        if not self.is_ready or self.model is None:
            # Fallback: Passive Limit at Best Bid/Ask
            return PlacementDetails(price_offset=0.0, aggression=0, fill_probability_est=0.5)
            
        # Construct state vector
        # [Spread, Imbalance, Volatility, Time Remaining]
        # Using placeholders for missing data
        obs = np.array([spread, imbalance, 0.0, 1.0], dtype=np.float32)
        
        action, _ = self.model.predict(obs, deterministic=True)
        
        price_offset = float(action["price_offset"][0]) if isinstance(action, dict) else float(action[0])
        aggression = int(action["aggression"]) if isinstance(action, dict) else int(action[1]) if len(action)>1 else 0
        
        return PlacementDetails(
            price_offset=price_offset,
            aggression=aggression,
            fill_probability_est=0.8 if aggression == 1 else 0.4
        )
